{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1c66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## Topic Modeling In N L P: Tf Idf - 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398bcab4-8f21-4211-a3da-9f0d3c3521c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.3.2)\n",
      "Requirement already satisfied: scipy==1.12 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: wordcloud in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.9.3)\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /home/codespace/.local/lib/python3.10/site-packages (from scipy==1.12->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: numexpr in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: funcy in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (1.4.1.post1)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis->-r requirements.txt (line 1)) (68.2.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim->-r requirements.txt (line 2)) (7.0.4)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud->-r requirements.txt (line 4)) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud->-r requirements.txt (line 4)) (3.8.3)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (4.66.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=1.0.0->pyLDAvis->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->pyLDAvis->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud->-r requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud->-r requirements.txt (line 4)) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud->-r requirements.txt (line 4)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud->-r requirements.txt (line 4)) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis->-r requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aa62a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 13/43: Load packages  ####\n",
    "\n",
    "# Module in standard library\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# 3rd party packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import models\n",
    "\n",
    "nltk.download([\"punkt\", \"stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9776c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================-\n",
    "#### Slide 14/43: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "# home_dir = Path(\".\").resolve()\n",
    "# main_dir = home_dir.parent.parent\n",
    "# print(main_dir)\n",
    "# data_dir = str(main_dir) + \"/data\"\n",
    "# print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250393d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             web_url  \\\n",
      "0  https://www.nytimes.com/reuters/2019/01/01/spo...   \n",
      "1  https://www.nytimes.com/reuters/2019/01/01/wor...   \n",
      "2  https://www.nytimes.com/aponline/2019/01/01/sp...   \n",
      "3  https://www.nytimes.com/2019/01/09/arts/design...   \n",
      "4  https://www.nytimes.com/aponline/2019/01/10/sp...   \n",
      "\n",
      "                                            headline  \\\n",
      "0  Kyrgios, Murray Power Into Second Round in Bri...   \n",
      "1  UK Police Treating Manchester Stabbing Attack ...   \n",
      "2  Former NFL Player Wiley Talks Playoffs on Podc...   \n",
      "3    After the Quake, Dana Schutz Gets Back to Work    \n",
      "4  Ogunbowale Helps Irish Beat Cardinals in 1-2 S...   \n",
      "\n",
      "                                             snippet  word_count  \\\n",
      "0  Nick Kyrgios started his Brisbane Open title d...         435   \n",
      "1  British police confirmed on Tuesday they were ...          81   \n",
      "2  Marcellus Wiley is still on the fence about le...         272   \n",
      "3  Still reckoning with the fallout from her Emme...        1540   \n",
      "4  As far as Arike Ogunbowale and coach Muffet Mc...        1059   \n",
      "\n",
      "               source type_of_material        date  id  \n",
      "0             Reuters             News  2019-01-01   8  \n",
      "1             Reuters             News  2019-01-01   9  \n",
      "2                  AP             News  2019-01-01  10  \n",
      "3  The New York Times             News  2019-01-09  11  \n",
      "4                  AP             News  2019-01-11  12  \n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 17/43: Load data  ####\n",
    "\n",
    "# Let's load and prepare the dataset for creating Document-Term Matrix\n",
    "df = pd.read_csv(\"data/NYT_article_data.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea17a17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 18/43: Check for NAs  ####\n",
    "\n",
    "# Print total number of NAs.\n",
    "print(df[\"snippet\"].isna().sum())\n",
    "# Drop NAs if any.\n",
    "df = df.dropna(subset=[\"snippet\"]).reset_index(drop=True)\n",
    "print(df[\"snippet\"].isna().sum())\n",
    "# Isolate the `snippet` column.\n",
    "df_text = df[\"snippet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2d6645",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# =================================================-\n",
    "#### Slide 19/43: Tokenization: split each document into words  ####\n",
    "\n",
    "# Tokenize each document into a large list of tokenized documents.\n",
    "df_tokenized = [word_tokenize(df_text[i]) for i in range(0, len(df_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81fa592",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'Kyrgios', 'started', 'his', 'Brisbane', 'Open', 'title', 'defense', 'with', 'a', 'battling', '7-6', '(', '5', ')', '5-7', '7-6', '(', '5', ')', 'victory', 'over', 'American', 'Ryan', 'Harrison', 'in', 'the', 'opening', 'round', 'on', 'Tuesday', '.']\n",
      "['nick', 'kyrgios', 'started', 'his', 'brisbane', 'open', 'title', 'defense', 'with', 'a']\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 21/43: Convert characters to lowercase  ####\n",
    "\n",
    "# Let's take a look at the first tokenized document\n",
    "document_words = df_tokenized[0]\n",
    "print(document_words)\n",
    "# 1. Convert to lowercase.\n",
    "document_words = [word.lower() for word in document_words]\n",
    "print(document_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bdb4b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['nick', 'kyrgios', 'started', 'brisbane', 'open', 'title', 'defense', 'battling', '7-6', '(']\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 22/43: Remove stop words  ####\n",
    "\n",
    "# 2. Remove stop words.\n",
    "# Get common English stop words.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words[:10])\n",
    "# Remove stop words.\n",
    "document_words = [word for word in document_words if not word in stop_words]\n",
    "print(document_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3507d9c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nick', 'kyrgios', 'started', 'brisbane', 'open', 'title', 'defense', 'battling', 'victory', 'american']\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 23/43: Remove non-alphabetical characters  ####\n",
    "\n",
    "# 3. Remove punctuation and any non-alphabetical characters.\n",
    "document_words = [word for word in document_words if word.isalpha()]\n",
    "print(document_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeed1c0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nick', 'kyrgio', 'start', 'brisban', 'open', 'titl', 'defens', 'battl', 'victori', 'american']\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 26/43: Stem words  ####\n",
    "\n",
    "# 4. Stem words.\n",
    "document_words = [PorterStemmer().stem(word) for word in document_words]\n",
    "print(document_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef0dff2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# =================================================-\n",
    "#### Slide 27/43: Clean the entire corpus  ####\n",
    "\n",
    "# Create a list for clean documents.\n",
    "df_clean = [None] * len(df_tokenized)\n",
    "# Create a list of word counts for each clean document.\n",
    "word_counts_per_document = [None] * len(df_tokenized)\n",
    "\n",
    "# Process words in all documents.\n",
    "for i in range(len(df_tokenized)):\n",
    "    # 1. Convert to lowercase.\n",
    "    df_clean[i] = [document.lower() for document in df_tokenized[i]]\n",
    "\n",
    "    # 2. Remove stop words.\n",
    "    df_clean[i] = [word for word in df_clean[i] if not word in stop_words]\n",
    "\n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    df_clean[i] = [word for word in df_clean[i] if word.isalpha()]\n",
    "\n",
    "    # 4. Stem words.\n",
    "    df_clean[i] = [PorterStemmer().stem(word) for word in df_clean[i]]\n",
    "\n",
    "    # Record the word count per document.\n",
    "    word_counts_per_document[i] = len(df_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a705b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# =================================================-\n",
    "#### Slide 28/43: Clean the entire corpus (cont'd)  ####\n",
    "\n",
    "# Convert word counts list and documents list to NumPy arrays.\n",
    "word_counts_array = np.array(word_counts_per_document)\n",
    "df_array = np.array(df_clean, dtype=object)\n",
    "\n",
    "# Find indices of all documents where there are greater than or equal to 5 words.\n",
    "valid_documents = np.where(word_counts_array >= 5)[0]\n",
    "\n",
    "# Subset the df_array to keep only those where there are at least 5 words.\n",
    "df_array = df_array[valid_documents]\n",
    "\n",
    "# Convert the array back to a list.\n",
    "df_clean = df_array.tolist()  # <- the processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f12f27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nick', 'kyrgio', 'start', 'brisban', 'open', 'titl', 'defens', 'battl', 'victori', 'american', 'ryan', 'harrison', 'open', 'round', 'tuesday'], ['british', 'polic', 'confirm', 'tuesday', 'treat', 'stab', 'attack', 'injur', 'three', 'peopl', 'manchest', 'victoria', 'train', 'station', 'terrorist', 'investig', 'search', 'address', 'cheetham', 'hill', 'area', 'citi']]\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 30/43: Data for TF-IDF matrix   ####\n",
    "\n",
    "print(df_clean[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc975a9-9b4d-4448-bb7f-ee96622cbec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['american',\n",
       " 'battl',\n",
       " 'brisban',\n",
       " 'defens',\n",
       " 'harrison',\n",
       " 'kyrgio',\n",
       " 'nick',\n",
       " 'open',\n",
       " 'round',\n",
       " 'ryan']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 32/43: Create a dictionary of counts   ####\n",
    "\n",
    "# Set the seed.\n",
    "np.random.seed(1)\n",
    "dictionary = gensim.corpora.Dictionary(df_clean)\n",
    "\n",
    "# The obtain the first 10 items of the dictionary\n",
    "from itertools import islice\n",
    "list(islice(dictionary.itervalues(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2a717f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 33/43: Create a dictionary of counts (cont'd)  ####\n",
    "\n",
    "dictionary.filter_extremes(no_below=4, no_above=0.5)\n",
    "\n",
    "# How many words are left in the dictionary?\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dbb64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 34/43: Document to bag-of-words  ####\n",
    "\n",
    "# We use a list comprehension to transform each doc within our df_clean object.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in df_clean]\n",
    "\n",
    "# Let's look at the first document.\n",
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca43f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"american\") appears 1 time.\n",
      "Word 1 (\"defens\") appears 1 time.\n",
      "Word 2 (\"open\") appears 2 time.\n",
      "Word 3 (\"round\") appears 1 time.\n",
      "Word 4 (\"start\") appears 1 time.\n",
      "Word 5 (\"tuesday\") appears 1 time.\n",
      "Word 6 (\"victori\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 35/43: Document to bag-of-words  ####\n",
    "\n",
    "# Isolate the first document.\n",
    "bow_doc_1 = bow_corpus[0]\n",
    "\n",
    "# Iterate through each dictionary item using the index.\n",
    "# Print out each actual word and how many times it appears.\n",
    "for i in range(len(bow_doc_1)):\n",
    "    print(\n",
    "        f'Word {bow_doc_1[i][0]} '\n",
    "        f'(\"{dictionary[bow_doc_1[i][0]]}\") '\n",
    "        f'appears {bow_doc_1[i][1]} time.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a544d5-2e61-4eee-ba00-48a1cedce7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.31942373876087665),\n",
       " (1, 0.3549009519669791),\n",
       " (2, 0.6118718565633235),\n",
       " (3, 0.3549009519669791),\n",
       " (4, 0.3059359282816618),\n",
       " (5, 0.22829905152454918),\n",
       " (6, 0.3549009519669791)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =================================================-\n",
    "#### Slide 37/43: Transform counts with TfidfModel  ####\n",
    "\n",
    "# This is the transformation.\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply the transformation to the entire corpus.\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# Preview TF-IDF scores for the first document.\n",
    "\n",
    "next(iter(corpus_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6119a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================-\n",
    "#### Slide 40/43: Exercise  ####\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
