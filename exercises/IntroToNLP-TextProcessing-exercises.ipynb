{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d4abea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T07:51:52.567021Z",
     "iopub.status.busy": "2024-04-10T07:51:52.564834Z",
     "iopub.status.idle": "2024-04-10T07:51:52.597202Z",
     "shell.execute_reply": "2024-04-10T07:51:52.587178Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## Intro To N L P: Text Processing : EXERCISES  ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f82e4",
   "metadata": {},
   "source": [
    "#### Exercise ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b28ab",
   "metadata": {},
   "source": [
    "#### Please refer to module 1 of IntroToNLP - TextProcessing for Tasks 1-3\n",
    "#### Task 1:\n",
    "##### Import all packages required for text processing.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f14d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f48f235f",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "##### Use `Path` module from `pathlib` to point `data_dir` to your data directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a6f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5740dd75",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "##### Read in the \"UN_agreement_titles.csv\" dataset and save the dataframe as `ex_df`.\n",
    "##### Remove all rows from the dataset where `title` is empty. \n",
    "##### Subset the `title` column from `ex_df` and call it `ex_df_text`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a15b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e52b7e",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of IntroToNLP - TextProcessing for Tasks 4-6\n",
    "#### Task 4:\n",
    "##### Tokenize all documents in `ex_df_text` into a list of tokenized documents and save it as `ex_df_tokenized`.\n",
    "##### For each tokenized document in `ex_df_tokenized`, do the following:\n",
    "##### Convert the tokens to lowercase.\n",
    "##### Get common English stopwords from `nltk.corpus` and remove them from tokenized document.\n",
    "##### Remove punctuation and all non-alphabetical characters.\n",
    "##### Perform stemming of tokens using `PorterStemmer()`.\n",
    "##### Save the number of words in each document as a list and call it `ex_word_counts_per_document`.\n",
    "##### Save the list of clean documents as `ex_df_clean`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebd8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d98fd07",
   "metadata": {},
   "source": [
    "#### Task 5:\n",
    "##### Inspect results obtained by performing Task 4.\n",
    "##### Plot a histogram for word counts per document and set bins to the number of unique values in the list.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d2322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176169d1",
   "metadata": {},
   "source": [
    "#### Task 6:\n",
    "##### Convert word counts list and documents list to NumPy arrays and call them `ex_word_counts_array` and `ex_df_array` respectively.\n",
    "##### Filter out all documents containing less than 4 words and save the filtered array as a list with the name `ex_df_clean`. Check how many valid documents are left.\n",
    "##### Use list comprehension to combine all words per document into one character string each. Save this list of lists as `ex_df_clean_list`. Hint: Use the `join()` function for converting words into a single string.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbccc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08271a59",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of IntroToNLP - TextProcessing for Tasks 7-9\n",
    "#### Task 7:\n",
    "##### Create a `CountVectorizer` object and use it to transform `ex_df_clean_list` into a Document-Term Matrix (DTM) and call it `ex_X`.\n",
    "##### Use the same `CountVectorizer` object to print the first 6 feature names.\n",
    "##### Convert the DTM `ex_X` into a Pandas DataFrame `ex_DTM` and set the column names as the feature names.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbb9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f996dc",
   "metadata": {},
   "source": [
    "#### Task 8:\n",
    "##### Sum the frequencies of each word in all documents and save the result as a dictionary `ex_corpus_freq_dist`.\n",
    "##### Convert this dictionary into an `nltk.FreqDist` object.\n",
    "##### Use the `nltk.FreqDist` object to plot the 50 most frequent words in the corpus.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc5866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20a30eea",
   "metadata": {},
   "source": [
    "#### Task 9:\n",
    "##### Take the 3rd document in `ex_df_clean_list` and create bi-gram and tri-gram for it. Hint: Use the `ngrams()` function from `nltk`.\n",
    "##### Construct a wordcloud of the corpus `ex_df_clean_list` and set `collocations=False`.\n",
    "##### Plot the wordcloud using bilinear interpolation.\n",
    "##### Turn off the plot axes.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb5673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
